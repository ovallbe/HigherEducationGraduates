
#These workings are for R Code

#Step 0: Create a folder with workings and data. 
if (!file.exists("HE_Grad")) {
  dir.create("HE_Grad")
}

setwd("./HE_Grad")

if(!file.exists("data")){dir.create("data")}


#Step 1: Download a file

#Create a folder

dateDownloaded <-format(Sys.time(),"%Y%m%d")
dateDownloaded


#About the date formats
#http://www.statmethods.net/input/dates.html

todayData <- paste("data",dateDownloaded)
if(!file.exists(todayData)){dir.create(todayData)}

###Add the folder data the datetime (concatenate the names)  <---------NEED TO DOWNLOAD ALL THE NAMES


#######Data sources
#Our next step is to create a list with all the urls of each file that we want. 
#First adress: This one has the URLs of 5 pages, one for field of study.  
# http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html

install.packages("knitr")

download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rpres')

download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rmd')

#Web scrapping: http://www.r-bloggers.com/web-scraping-in-r/
#http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/
#Main Functions
#readLines() --> Creates a character vector, Lenght+ number of lines in the html code, 
#Hmisc package
install.packages("Hmisc")
#substring.location(...) ; grep(...) ; substr(...) ; strsplit(...

#I clean the environment data
rm(list=ls())

#I have to look for the links, in html all that is between the commas after href=""







fileUrl <- "URLXXXXXXXx"
download.file(fileUrl,destfile=(./data/NAME.xlsx, method = "curl") 
  CHECH METHOD CURL
  list.files("./data")


  
