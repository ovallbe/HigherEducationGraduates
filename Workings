The R function system.time will report the amount of time any operation in R uses

install.packages("stringr")
install.packages("knitr")
install.packages("ggplot2")
library("stringr")

#These workings are for R Code

#Step 0: Create a folder with workings and data. 
if (!file.exists("HE_Grad")) {
  dir.create("HE_Grad")
}
list.files()
list.dirs()
setwd("./HE_Grad")
setwd("./data")
setwd("C:/Users/NAME/Documents/R/HE_Grad")
if(!file.exists("data")){dir.create("data")}



#Step 1: Download a file

#Create a folder

dateDownloaded <-format(Sys.time(),"%Y%m%d")
dateDownloaded

#About the date formats
#http://www.statmethods.net/input/dates.html

todayData <- paste("data",dateDownloaded)
if(!file.exists(todayData)){dir.create(todayData)}

mainDir <- getwd()
setwd(file.path(mainDir,todayData))
getwd()

download.maybe <- function(url, refetch=FALSE, path=".") {
  dest <- file.path(path,basename(url))
if (refetch||!file.exists(dest))
  download.file(url,dest)
dest}
#######http://nicercode.github.io/guides/repeating-things/

###Add the folder data the datetime (concatenate the names)  <---------NEED TO DOWNLOAD ALL THE NAMES


#######Data sources
#Our next step is to create a list with all the urls of each file that we want. 
#First adress: This one has the URLs of 5 pages, one for field of study.  
# http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html


download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rpres')

download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rmd')
######REGULAR EXPRESSIONS CHEAT FACT
#http://www.endmemo.com/program/R/grep.php
#https://developer.gnome.org/glib/stable/glib-regex-syntax.html
#http://stackoverflow.com/questions/2443127/how-can-i-use-r-rcurl-xml-packages-to-scrape-this-webpage
#http://stackoverflow.com/questions/3746256/extract-links-from-webpage-using-r
#links<-strsplit(selected_lines,"a href=")[[1]]

#Web scrapping: http://www.r-bloggers.com/web-scraping-in-r/
#http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/
#Main Functions
#readLines() --> Creates a character vector, Lenght+ number of lines in the html code, 
#Hmisc package
install.packages("Hmisc")
#substring.location(...) ; grep(...) ; substr(...) ; strsplit(...

#I clean the environment data
rm(list=ls())

#I have to look for the links, in html all that is between the commas after href=""
#I have to recover my notes of "Regular Expressions" of the "Computer for Data Analysis" course..

#I extract with substring, but I need to identify the exact point of each string. 
substr("abcdef", 2, 4)
substring("abcdef", 1:6, 1:6)



library("stringr")
con = url("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html")
htmlCode=readLines(con)
close(con)
First_page = htmlCode
links_lines <-grep("href=",First_page)
First_page[links_lines]
selected_lines  <- First_page[links_lines]
nchar( selected_lines[11])
str_locate(selected_lines,"href=")
line_position <-str_locate(selected_lines,"href=")
substr(selected_lines[11],line_position[11,1],line_position[11,2])
 str_locate(selected_lines,".html")
 
my_lines <- as.data.frame(selected_lines)
 
 ## <<<<-------- Identify the webs and make a vector withall the links

> sel_lines_integ <- toString(selected_lines)
> sel_lines_integ

> string_position <-str_locate(sel_lines_integ,"href=")
> string_position
nchar(my_lines[1,1])
Error in nchar(my_lines[1, 1]) : 'nchar()' requires a character vector

fileUrl <- "URLXXXXXXXx"
download.file(fileUrl,destfile=(./data/NAME.xlsx, method = "curl") 
  CHECH METHOD CURL
  list.files("./data")
  
  
  ###### Option 1
  > con = url("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html")
> First_page=readLines(con)
> close(con)
> links_lines <-grep("href=",First_page)
> selected_lines  <- First_page[links_lines]
list_sl <- as.list(selected_lines)
list2 <- sub(".*href=","", list_sl)
list3 <- sub("><.*","",list2)
list4 <- sub("\">.*","", list3) 
#NOTE THAT I AM AVOIDING THE LABELS FOR THE TIME BEEING (I EXPECTED TO CREATE A data frame with the URL in one column and the label in the other)
list6 <- list5[7:11]
con2 <- url(paste("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio","/",list6[1]))
con2
substr(list6[1],0,nchar(list6[1])-5)

I want to create a frame with the connection, and the name
INFORMEST_TITULACIO changes

con2 <- url("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/socials.html")

http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/humanitats.html
http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/socials.html
http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/experimentals.html
http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/salut.html
http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/tecnica.html

socials_page=readLines(con2)


htmlCode2=readLines(con2)
socials_page=readLines(con2)
soc_links_lines <-grep("href=",socials_page)
soc_links_lines
socials_lines  <- socials_page[soc_links_lines]

socials_lines[15]

#<a href=\"informes_titulacio/socials/Dipl_Mestre_Ed_Primaria.xls\
#Need to isolate everything btween href and xls, but.. there are zls after (xls.gif) and href befdore (for the pdf!!)


#

#other options
#sapply(strsplit(string, ":"), "[", 2)

length(socials_page)
######
NOVEMBER WORKINGS

> con2 <- url("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/socials.html")
> socials_page=readLines(con2)
> close(con2)
> head(socials_page)
(load stringr)
> ?str_match

library(stringr)
socials_page=readLines(con2)
socials_frame<-data.frame(socials_page)
test1<-str_match(socials_frame[,1], '<a href="([^"]*).xls')
 test2<-subset(test1[,2],!is.na(test1[,2]))
 test2_df<-data.frame(test2)
test3<-data.frame(num=1:30,url="http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/",ext=".xls")
test3$file <-test2_df[1]
test4 <- paste(test3[2,2],test2_df[2,1],test3[2,3])
test5 <- data.frame(paste(test3[,2],test2_df[,1],test3[,3]))
lenght(test5)
I need to delete the spaces
test5 <-paste(test3[,2],test2_df[,1],test3[,3])
test6 <- sub(" ","",test5)
test7 <- sub(" ","",test6)
#test8 <-data.frame(test7) ## NOOO I need a character vector


##DOWNLOADING THE FILES
mainDir <- getwd()
setwd(file.path(mainDir,todayData))
getwd()

download.maybe <- function(url, refetch=FALSE, path=".") {
  dest <- file.path(path,basename(url))
if (refetch||!file.exists(dest))
  download.file(url,dest)
dest}
#######http://nicercode.github.io/guides/repeating-things/
thispath <- getwd()


myfunction <- function(arg1, arg2, ... ){
statements
return(object)
}
###I DID IT!!!!
downloadall <- function(url,path=".") {
dest <- file.path(path,basename(url))
download.file(url,dest)
dest
}


sapply(test7,download.maybe,path=thispath) ## IT WORKS!!
 test7b <-sapply(test7,basename)
 
sapply(test7,download.file,destfile=file.path(thispath,test7c[,1]))
test7c <- data.frame(test7b)

sapply(test7,download.file,destfile=file.path(thispath,file.path(path,basename(test7)))
sapply(test7,download.file,destfile=file.path(thispath,file.path(thispath,basename(test7))))





