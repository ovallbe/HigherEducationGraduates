The R function system.time will report the amount of time any operation in R uses

install.packages("stringr")
install.packages("knitr")
install.packages("ggplot2")
library("stringr")

#These workings are for R Code

#Step 0: Create a folder with workings and data. 
if (!file.exists("HE_Grad")) {
  dir.create("HE_Grad")
}

setwd("./HE_Grad")

if(!file.exists("data")){dir.create("data")}


#Step 1: Download a file

#Create a folder

dateDownloaded <-format(Sys.time(),"%Y%m%d")
dateDownloaded


#About the date formats
#http://www.statmethods.net/input/dates.html

todayData <- paste("data",dateDownloaded)
if(!file.exists(todayData)){dir.create(todayData)}

###Add the folder data the datetime (concatenate the names)  <---------NEED TO DOWNLOAD ALL THE NAMES


#######Data sources
#Our next step is to create a list with all the urls of each file that we want. 
#First adress: This one has the URLs of 5 pages, one for field of study.  
# http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html


download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rpres')

download.file('https://raw.github.com/fredheir/WebScraping/master/Lecture1/p1.Rpres',
destfile='p1.Rmd')

#Web scrapping: http://www.r-bloggers.com/web-scraping-in-r/
#http://www.r-bloggers.com/using-apply-sapply-lapply-in-r/
#Main Functions
#readLines() --> Creates a character vector, Lenght+ number of lines in the html code, 
#Hmisc package
install.packages("Hmisc")
#substring.location(...) ; grep(...) ; substr(...) ; strsplit(...

#I clean the environment data
rm(list=ls())

#I have to look for the links, in html all that is between the commas after href=""
#I have to recover my notes of "Regular Expressions" of the "Computer for Data Analysis" course..

#I extract with substring, but I need to identify the exact point of each string. 
substr("abcdef", 2, 4)
substring("abcdef", 1:6, 1:6)


 str_locate(selected_lines,"href=")

con = url("http://www.aqu.cat/uploads/insercio_laboral/enquesta2014/informes_titulacio.html")
htmlCode=readLines(con)
close(con)
First_page = htmlCode
links_lines <-grep("href=",First_page)
First_page[links_lines]
selected_lines  <- First_page[links_lines]
nchar( selected_lines[11])
str_locate(selected_lines,"href=")




fileUrl <- "URLXXXXXXXx"
download.file(fileUrl,destfile=(./data/NAME.xlsx, method = "curl") 
  CHECH METHOD CURL
  list.files("./data")


  
